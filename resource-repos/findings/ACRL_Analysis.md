# ACRL (Assetto Corsa Reinforcement Learning) - Comprehensive Analysis

## Project Overview

ACRL is a Python-based reinforcement learning project that uses the Soft Actor-Critic (SAC) algorithm to train an autonomous driving agent in the Assetto Corsa racing simulator. The project was developed as a bachelor assignment by Jurre de Ruiter at the University of Twente.

## Architecture Overview

The project consists of two main components:

### 1. Assetto Corsa App (`ACRL/`)
- **Purpose**: Runs inside Assetto Corsa and communicates real-time game data
- **Language**: Python 3.3.5 (required by Assetto Corsa)
- **Key Files**:
  - `ACRL.py`: Main app entry point with GUI and socket communication
  - `ac_api/`: Data collection modules for car, input, lap, and session information

### 2. Standalone Python Project (`standalone/`)
- **Purpose**: Runs outside Assetto Corsa, implements SAC algorithm and controls the car
- **Language**: Python 3.10.11
- **Key Files**:
  - `main.py`: Entry point for training
  - `ac_socket.py`: Socket communication with AC app
  - `ac_controller.py`: Virtual controller using vgamepad
  - `sac/`: SAC algorithm implementation

## Implementation Status

### ‚úÖ Fully Implemented Components

#### 1. Data Collection System
- **Car Information**: Speed, position, velocity, world coordinates
- **Input Information**: Throttle, brake, steering inputs
- **Lap Information**: Lap times, lap count, track progress
- **Session Information**: Track length, car count, position
- **Tire Information**: Tire wear, temperature, pressure

#### 2. Socket Communication
- **Protocol**: TCP socket on localhost:65431
- **Data Format**: Key-value pairs separated by commas
- **Real-time**: Continuous data streaming during training

#### 3. SAC Algorithm Implementation
- **Actor Network**: Squashed Gaussian MLP with tanh activation
- **Critic Networks**: Two Q-functions for stability
- **Replay Buffer**: FIFO experience replay (1M capacity)
- **Hyperparameters**: Configurable learning rate, batch size, etc.

#### 4. Environment Interface
- **Gymnasium Integration**: Custom environment following Gym standards
- **Observation Space**: 10-dimensional state vector
- **Action Space**: 2-dimensional continuous actions (throttle/brake, steering)
- **Reward Functions**: Multiple reward schemes implemented

#### 5. Virtual Controller
- **Library**: vgamepad for Xbox 360 controller emulation
- **Controls**: Throttle, brake, steering inputs
- **Reset Functionality**: F10 key for car respawn

#### 6. Track Analysis
- **Spline Points**: Pre-computed center line for Silverstone 1967
- **Distance Calculation**: Distance from car to center line
- **Heading Error**: Angle between car direction and track direction

### üîß Partially Implemented Components

#### 1. Reward Functions
Five different reward schemes are implemented:
- `_get_reward_1`: Speed-based only
- `_get_reward_2`: Speed + progress
- `_get_reward_3`: Delta progress only
- `_get_reward_4`: Speed + delta progress
- `_get_reward_5`: Speed + angle + distance from center

**Status**: All implemented but only `_get_reward_5` is actively used

#### 2. Model Persistence
- **Save/Load**: Framework exists but implementation incomplete
- **Checkpointing**: Basic structure present
- **State Restoration**: TODO comment indicates incomplete implementation

### ‚ùå Missing/Incomplete Components

#### 1. Multi-GPU Support
- **MPI Integration**: Basic setup but not fully utilized
- **Distributed Training**: Framework present but not implemented

#### 2. Advanced Features
- **Curriculum Learning**: Not implemented
- **Multi-track Support**: Only Silverstone 1967 configured
- **Multi-car Support**: Only Ferrari 458 GT2 configured
- **Advanced Logging**: Basic logging but limited analytics

## Technical Details

### State Representation (10 dimensions)
1. `track_progress`: Progress on track [0.0, 1.0]
2. `speed_kmh`: Speed in km/h [0.0, max_speed]
3. `world_loc_x`: World X coordinate [-2000.0, 2000.0]
4. `world_loc_y`: World Y coordinate [-2000.0, 2000.0]
5. `world_loc_z`: World Z coordinate [-2000.0, 2000.0]
6. `lap_invalid`: Lap validity flag [0.0, 1.0]
7. `lap_count`: Current lap number [1.0, 2.0]
8. `previous_track_progress`: Previous progress [0.0, 1.0]
9. `heading_error`: Angle error [0.0, 2œÄ]
10. `dist_offcenter`: Distance from center line [0.0, 500.0]

### Action Space (2 dimensions)
1. `throttle_brake`: Combined throttle/brake [-1.0, 1.0]
2. `steering`: Steering angle [-1.0, 1.0]

### Network Architecture
- **Actor**: MLP with [obs_dim, 256, 256, act_dim]
- **Critic**: MLP with [obs_dim + act_dim, 256, 256, 1]
- **Activation**: ReLU for hidden layers, tanh for actor output
- **Device**: Automatic GPU detection and usage

### Training Configuration
```python
hyperparams = {
    "gamma": 0.99,           # Discount factor
    "polyak": 0.999,         # Soft target update
    "lr": 1e-3,              # Learning rate
    "alpha": 0.2,            # Entropy regularization
    "batch_size": 32,        # Batch size
    "n_episodes": 10000,     # Training episodes
    "update_after": 1000,    # Steps before first update
    "update_every": 50,      # Update frequency
    "start_steps": 10000,    # Random action steps
    "replay_size": int(1e6)  # Replay buffer size
}
```

## Dependencies

### Core Requirements
- `torch`: PyTorch for neural networks
- `numpy`: Numerical computations
- `gymnasium`: RL environment interface
- `vgamepad`: Virtual controller
- `keyboard`: Keyboard input handling
- `matplotlib`: Plotting and visualization
- `mpi4py`: MPI for distributed training
- `joblib`: Parallel processing

### Assetto Corsa Requirements
- Python 3.3.5 (specific version required by AC)
- Microsoft Visual C++ 2015-2019 Redistributable
- Microsoft Messaging Passing Interface (MS-MPI) v10.1.3

## Setup and Installation

### Prerequisites
1. Assetto Corsa (Steam)
2. Content Manager (free extension)
3. Python 3.3.5 for AC app
4. Python 3.10.11 for standalone training
5. Required system libraries (Visual C++, MS-MPI)

### Installation Steps
1. Copy `ACRL/` folder to `apps/python/` in AC installation
2. Enable ACRL app in AC settings
3. Install Python dependencies: `pip install -r requirements.txt`
4. Configure AC session (Silverstone 1967, Ferrari 458 GT2)
5. Run `standalone/main.py` to start training

## Limitations and Issues

### Technical Limitations
1. **Single Track**: Only configured for Silverstone 1967
2. **Single Car**: Only Ferrari 458 GT2 supported
3. **Windows Dependency**: vgamepad requires Windows
4. **Python Version**: AC requires specific Python 3.3.5
5. **Performance**: No GPU optimization for real-time inference

### Implementation Issues
1. **Model Loading**: Incomplete implementation (TODO comment)
2. **Error Handling**: Limited error recovery mechanisms
3. **Logging**: Basic logging without advanced analytics
4. **Configuration**: Hard-coded parameters in many places

### Scalability Concerns
1. **Single Agent**: No multi-agent training
2. **Single Track**: No multi-track generalization
3. **Limited Exploration**: Basic exploration strategy
4. **No Transfer Learning**: Each track requires separate training

## Performance Characteristics

### Training Performance
- **Episode Length**: 1000 steps maximum (TimeLimit wrapper)
- **Update Frequency**: Every 50 environment steps
- **Batch Size**: 32 samples per update
- **Replay Buffer**: 1M experience capacity

### Real-time Performance
- **Socket Communication**: ~30 FPS target
- **Action Frequency**: Continuous real-time control
- **Latency**: Minimal due to localhost communication

## Recommendations for Improvement

### Short-term Improvements
1. **Complete Model Loading**: Fix the TODO in model restoration
2. **Better Error Handling**: Add robust error recovery
3. **Configuration Files**: Move hard-coded parameters to config files
4. **Enhanced Logging**: Add comprehensive training analytics

### Medium-term Enhancements
1. **Multi-track Support**: Extend to other tracks
2. **Curriculum Learning**: Implement progressive difficulty
3. **Advanced Rewards**: Experiment with more sophisticated reward functions
4. **Performance Optimization**: GPU acceleration for real-time inference

### Long-term Goals
1. **Transfer Learning**: Pre-trained models for new tracks
2. **Multi-agent Training**: Competitive/cooperative scenarios
3. **Advanced Algorithms**: PPO, TD3, or other RL algorithms
4. **Real-world Transfer**: Sim-to-real transfer capabilities

## Conclusion

ACRL is a well-structured and functional implementation of SAC for autonomous racing in Assetto Corsa. The core components are fully implemented and working, providing a solid foundation for reinforcement learning research in racing environments. While there are some limitations and incomplete features, the project demonstrates good software engineering practices and provides a complete pipeline from data collection to model training and deployment.

The project successfully bridges the gap between simulation and real-world racing applications, making it a valuable resource for autonomous driving research and education. 